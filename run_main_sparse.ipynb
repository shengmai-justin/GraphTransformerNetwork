{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5d962d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install environment: torch matplotlib\n",
    "!pip install torch\n",
    "!pip install matplotlib\n",
    "# install torch geometric and related packages\n",
    "# also cab be installed through conda:\n",
    "# !conda install -c pyg pyg\n",
    "!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-1.11.0+cpu.html!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b316f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def uniform(size, tensor):\n",
    "    bound = 1.0 / math.sqrt(size)\n",
    "    if tensor is not None:\n",
    "        tensor.data.uniform_(-bound, bound)\n",
    "\n",
    "\n",
    "def kaiming_uniform(tensor, fan, a):\n",
    "    bound = math.sqrt(6 / ((1 + a**2) * fan))\n",
    "    if tensor is not None:\n",
    "        tensor.data.uniform_(-bound, bound)\n",
    "\n",
    "\n",
    "def glorot(tensor):\n",
    "    stdv = math.sqrt(6.0 / (tensor.size(-2) + tensor.size(-1)))\n",
    "    if tensor is not None:\n",
    "        tensor.data.uniform_(-stdv, stdv)\n",
    "\n",
    "\n",
    "def zeros(tensor):\n",
    "    if tensor is not None:\n",
    "        tensor.data.fill_(0)\n",
    "\n",
    "\n",
    "def ones(tensor):\n",
    "    if tensor is not None:\n",
    "        tensor.data.fill_(1)\n",
    "\n",
    "\n",
    "def reset(nn):\n",
    "    def _reset(item):\n",
    "        if hasattr(item, 'reset_parameters'):\n",
    "            item.reset_parameters()\n",
    "\n",
    "    if nn is not None:\n",
    "        if hasattr(nn, 'children') and len(list(nn.children())) > 0:\n",
    "            for item in nn.children():\n",
    "                _reset(item)\n",
    "        else:\n",
    "            _reset(nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25a277b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import torch\n",
    "# comment the following import because torch_geometric.utlis does not have the scatter in latest version\n",
    "# instead, we can use torch.scatter(...,reduce='add') to get the same result\n",
    "#from torch_geometric.utils import scatter_\n",
    "\n",
    "special_args = [\n",
    "    'edge_index', 'edge_index_i', 'edge_index_j', 'size', 'size_i', 'size_j'\n",
    "]\n",
    "__size_error_msg__ = ('All tensors which should get mapped to the same source '\n",
    "                      'or target nodes must be of same size in dimension 0.')\n",
    "\n",
    "\n",
    "class MessagePassing(torch.nn.Module):\n",
    "    r\"\"\"Base class for creating message passing layers\n",
    "    .. math::\n",
    "        \\mathbf{x}_i^{\\prime} = \\gamma_{\\mathbf{\\Theta}} \\left( \\mathbf{x}_i,\n",
    "        \\square_{j \\in \\mathcal{N}(i)} \\, \\phi_{\\mathbf{\\Theta}}\n",
    "        \\left(\\mathbf{x}_i, \\mathbf{x}_j,\\mathbf{e}_{i,j}\\right) \\right),\n",
    "    where :math:`\\square` denotes a differentiable, permutation invariant\n",
    "    function, *e.g.*, sum, mean or max, and :math:`\\gamma_{\\mathbf{\\Theta}}`\n",
    "    and :math:`\\phi_{\\mathbf{\\Theta}}` denote differentiable functions such as\n",
    "    MLPs.\n",
    "    See `here <https://rusty1s.github.io/pytorch_geometric/build/html/notes/\n",
    "    create_gnn.html>`__ for the accompanying tutorial.\n",
    "    Args:\n",
    "        aggr (string, optional): The aggregation scheme to use\n",
    "            (:obj:`\"add\"`, :obj:`\"mean\"` or :obj:`\"max\"`).\n",
    "            (default: :obj:`\"add\"`)\n",
    "        flow (string, optional): The flow direction of message passing\n",
    "            (:obj:`\"source_to_target\"` or :obj:`\"target_to_source\"`).\n",
    "            (default: :obj:`\"source_to_target\"`)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, aggr='add', flow='source_to_target'):\n",
    "        super(MessagePassing, self).__init__()\n",
    "\n",
    "        self.aggr = aggr\n",
    "        assert self.aggr in ['add', 'mean', 'max']\n",
    "\n",
    "        self.flow = flow\n",
    "        assert self.flow in ['source_to_target', 'target_to_source']\n",
    "\n",
    "        self.__message_args__ = inspect.getfullargspec(self.message)[0][1:]\n",
    "        self.__special_args__ = [(i, arg)\n",
    "                                 for i, arg in enumerate(self.__message_args__)\n",
    "                                 if arg in special_args]\n",
    "        self.__message_args__ = [\n",
    "            arg for arg in self.__message_args__ if arg not in special_args\n",
    "        ]\n",
    "        self.__update_args__ = inspect.getfullargspec(self.update)[0][2:]\n",
    "\n",
    "    def propagate(self, edge_index, size=None, **kwargs):\n",
    "        r\"\"\"The initial call to start propagating messages.\n",
    "        Args:\n",
    "            edge_index (Tensor): The indices of a general (sparse) assignment\n",
    "                matrix with shape :obj:`[N, M]` (can be directed or\n",
    "                undirected).\n",
    "            size (list or tuple, optional): The size :obj:`[N, M]` of the\n",
    "                assignment matrix. If set to :obj:`None`, the size is tried to\n",
    "                get automatically inferrred. (default: :obj:`None`)\n",
    "            **kwargs: Any additional data which is needed to construct messages\n",
    "                and to update node embeddings.\n",
    "        \"\"\"\n",
    "\n",
    "        size = [None, None] if size is None else list(size)\n",
    "        assert len(size) == 2\n",
    "\n",
    "        i, j = (0, 1) if self.flow == 'target_to_source' else (1, 0)\n",
    "        ij = {\"_i\": i, \"_j\": j}\n",
    "\n",
    "        message_args = []\n",
    "        for arg in self.__message_args__:\n",
    "            if arg[-2:] in ij.keys():\n",
    "                tmp = kwargs[arg[:-2]]\n",
    "                if tmp is None:  # pragma: no cover\n",
    "                    message_args.append(tmp)\n",
    "                else:\n",
    "                    idx = ij[arg[-2:]]\n",
    "                    if isinstance(tmp, tuple) or isinstance(tmp, list):\n",
    "                        assert len(tmp) == 2\n",
    "                        if size[1 - idx] is None:\n",
    "                            size[1 - idx] = tmp[1 - idx].size(0)\n",
    "                        if size[1 - idx] != tmp[1 - idx].size(0):\n",
    "                            raise ValueError(__size_error_msg__)\n",
    "                        tmp = tmp[idx]\n",
    "\n",
    "                    if size[idx] is None:\n",
    "                        size[idx] = tmp.size(0)\n",
    "                    if size[idx] != tmp.size(0):\n",
    "                        raise ValueError(__size_error_msg__)\n",
    "\n",
    "                    tmp = torch.index_select(tmp, 0, edge_index[idx])\n",
    "                    message_args.append(tmp)\n",
    "            else:\n",
    "                message_args.append(kwargs[arg])\n",
    "\n",
    "        size[0] = size[1] if size[0] is None else size[0]\n",
    "        size[1] = size[0] if size[1] is None else size[1]\n",
    "\n",
    "        kwargs['edge_index'] = edge_index\n",
    "        kwargs['size'] = size\n",
    "\n",
    "        for (idx, arg) in self.__special_args__:\n",
    "            if arg[-2:] in ij.keys():\n",
    "                message_args.insert(idx, kwargs[arg[:-2]][ij[arg[-2:]]])\n",
    "            else:\n",
    "                message_args.insert(idx, kwargs[arg])\n",
    "\n",
    "        update_args = [kwargs[arg] for arg in self.__update_args__]\n",
    "\n",
    "        out = self.message(*message_args)\n",
    "        #out = scatter_(self.aggr, out, edge_index[i], dim_size=size[i]) #out = input, edge_index = index, \n",
    "        out = torch.scatter(size[i],edge_index[i], out, reduce='add')\n",
    "        out = self.update(out, *update_args)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def message(self, x_j):  # pragma: no cover\n",
    "        r\"\"\"Constructs messages in analogy to :math:`\\phi_{\\mathbf{\\Theta}}`\n",
    "        for each edge in :math:`(i,j) \\in \\mathcal{E}`.\n",
    "        Can take any argument which was initially passed to :meth:`propagate`.\n",
    "        In addition, features can be lifted to the source node :math:`i` and\n",
    "        target node :math:`j` by appending :obj:`_i` or :obj:`_j` to the\n",
    "        variable name, *.e.g.* :obj:`x_i` and :obj:`x_j`.\"\"\"\n",
    "\n",
    "        return x_j\n",
    "\n",
    "    def update(self, aggr_out):  # pragma: no cover\n",
    "        r\"\"\"Updates node embeddings in analogy to\n",
    "        :math:`\\gamma_{\\mathbf{\\Theta}}` for each node\n",
    "        :math:`i \\in \\mathcal{V}`.\n",
    "        Takes in the output of aggregation as first argument and any argument\n",
    "        which was initially passed to :meth:`propagate`.\"\"\"\n",
    "\n",
    "        return aggr_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5a8f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gcn\n",
    "import torch\n",
    "from torch.nn import Parameter\n",
    "from torch_scatter import scatter_add\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import remove_self_loops, add_self_loops\n",
    "\n",
    "#from inits import glorot, zeros\n",
    "#import pdb\n",
    "\n",
    "class GCNConv(MessagePassing):\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 improved=False,\n",
    "                 cached=False,\n",
    "                 bias=True):\n",
    "        super(GCNConv, self).__init__('add')\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.improved = improved\n",
    "        self.cached = cached\n",
    "        self.cached_result = None\n",
    "\n",
    "        self.weight = Parameter(torch.Tensor(in_channels, out_channels))\n",
    "\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        glorot(self.weight)\n",
    "        zeros(self.bias)\n",
    "        self.cached_result = None\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def norm(edge_index, num_nodes, edge_weight, improved=False, dtype=None):\n",
    "        if edge_weight is None:\n",
    "            edge_weight = torch.ones((edge_index.size(1), ),\n",
    "                                     dtype=dtype,\n",
    "                                     device=edge_index.device)\n",
    "        edge_weight = edge_weight.view(-1)\n",
    "        assert edge_weight.size(0) == edge_index.size(1)\n",
    "\n",
    "        edge_index, edge_weight = remove_self_loops(edge_index, edge_weight)\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=num_nodes)\n",
    "        loop_weight = torch.full((num_nodes, ),\n",
    "                                 1 if not improved else 2,\n",
    "                                 dtype=edge_weight.dtype,\n",
    "                                 device=edge_weight.device)\n",
    "        edge_weight = torch.cat([edge_weight, loop_weight], dim=0)\n",
    "\n",
    "        row, col = edge_index\n",
    "        \n",
    "        deg = scatter_add(edge_weight, col, dim=0, dim_size=num_nodes) #edge_weight=src, index=col, dim=0,\n",
    "        #deg = torch.scatter_add()\n",
    "        deg_inv_sqrt = deg.pow(-1)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "\n",
    "        return edge_index, deg_inv_sqrt[col] * edge_weight\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        \"\"\"\"\"\"\n",
    "        x = torch.matmul(x, self.weight)\n",
    "\n",
    "        if not self.cached or self.cached_result is None:\n",
    "            edge_index, norm = self.norm(edge_index, x.size(0), edge_weight,\n",
    "                                         self.improved, x.dtype)\n",
    "            self.cached_result = edge_index, norm\n",
    "        edge_index, norm = self.cached_result\n",
    "\n",
    "        return self.propagate(edge_index, x=x, norm=norm)\n",
    "\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        if self.bias is not None:\n",
    "            aggr_out = aggr_out + self.bias\n",
    "        return aggr_out\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels,\n",
    "                                   self.out_channels)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077ee3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils.py\n",
    "from __future__ import division\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def accuracy(pred, target):\n",
    "    r\"\"\"Computes the accuracy of correct predictions.\n",
    "\n",
    "    Args:\n",
    "        pred (Tensor): The predictions.\n",
    "        target (Tensor): The targets.\n",
    "\n",
    "    :rtype: int\n",
    "    \"\"\"\n",
    "    return (pred == target).sum().item() / target.numel()\n",
    "\n",
    "\n",
    "\n",
    "def true_positive(pred, target, num_classes):\n",
    "    r\"\"\"Computes the number of true positive predictions.\n",
    "\n",
    "    Args:\n",
    "        pred (Tensor): The predictions.\n",
    "        target (Tensor): The targets.\n",
    "        num_classes (int): The number of classes.\n",
    "\n",
    "    :rtype: :class:`LongTensor`\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for i in range(num_classes):\n",
    "        out.append(((pred == i) & (target == i)).sum())\n",
    "\n",
    "    return torch.tensor(out)\n",
    "\n",
    "\n",
    "\n",
    "def true_negative(pred, target, num_classes):\n",
    "    r\"\"\"Computes the number of true negative predictions.\n",
    "\n",
    "    Args:\n",
    "        pred (Tensor): The predictions.\n",
    "        target (Tensor): The targets.\n",
    "        num_classes (int): The number of classes.\n",
    "\n",
    "    :rtype: :class:`LongTensor`\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for i in range(num_classes):\n",
    "        out.append(((pred != i) & (target != i)).sum())\n",
    "\n",
    "    return torch.tensor(out)\n",
    "\n",
    "\n",
    "\n",
    "def false_positive(pred, target, num_classes):\n",
    "    r\"\"\"Computes the number of false positive predictions.\n",
    "\n",
    "    Args:\n",
    "        pred (Tensor): The predictions.\n",
    "        target (Tensor): The targets.\n",
    "        num_classes (int): The number of classes.\n",
    "\n",
    "    :rtype: :class:`LongTensor`\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for i in range(num_classes):\n",
    "        out.append(((pred == i) & (target != i)).sum())\n",
    "\n",
    "    return torch.tensor(out)\n",
    "\n",
    "\n",
    "\n",
    "def false_negative(pred, target, num_classes):\n",
    "    r\"\"\"Computes the number of false negative predictions.\n",
    "\n",
    "    Args:\n",
    "        pred (Tensor): The predictions.\n",
    "        target (Tensor): The targets.\n",
    "        num_classes (int): The number of classes.\n",
    "\n",
    "    :rtype: :class:`LongTensor`\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for i in range(num_classes):\n",
    "        out.append(((pred != i) & (target == i)).sum())\n",
    "\n",
    "    return torch.tensor(out)\n",
    "\n",
    "\n",
    "\n",
    "def precision(pred, target, num_classes):\n",
    "    r\"\"\"Computes the precision:\n",
    "    :math:`\\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FP}}`.\n",
    "\n",
    "    Args:\n",
    "        pred (Tensor): The predictions.\n",
    "        target (Tensor): The targets.\n",
    "        num_classes (int): The number of classes.\n",
    "\n",
    "    :rtype: :class:`Tensor`\n",
    "    \"\"\"\n",
    "    tp = true_positive(pred, target, num_classes).to(torch.float)\n",
    "    fp = false_positive(pred, target, num_classes).to(torch.float)\n",
    "\n",
    "    out = tp / (tp + fp)\n",
    "    out[torch.isnan(out)] = 0\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "def recall(pred, target, num_classes):\n",
    "    r\"\"\"Computes the recall:\n",
    "    :math:`\\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FN}}`.\n",
    "\n",
    "    Args:\n",
    "        pred (Tensor): The predictions.\n",
    "        target (Tensor): The targets.\n",
    "        num_classes (int): The number of classes.\n",
    "\n",
    "    :rtype: :class:`Tensor`\n",
    "    \"\"\"\n",
    "    tp = true_positive(pred, target, num_classes).to(torch.float)\n",
    "    fn = false_negative(pred, target, num_classes).to(torch.float)\n",
    "\n",
    "    out = tp / (tp + fn)\n",
    "    out[torch.isnan(out)] = 0\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "def f1_score(pred, target, num_classes):\n",
    "    r\"\"\"Computes the :math:`F_1` score:\n",
    "    :math:`2 \\cdot \\frac{\\mathrm{precision} \\cdot \\mathrm{recall}}\n",
    "    {\\mathrm{precision}+\\mathrm{recall}}`.\n",
    "\n",
    "    Args:\n",
    "        pred (Tensor): The predictions.\n",
    "        target (Tensor): The targets.\n",
    "        num_classes (int): The number of classes.\n",
    "\n",
    "    :rtype: :class:`Tensor`\n",
    "    \"\"\"\n",
    "    prec = precision(pred, target, num_classes)\n",
    "    rec = recall(pred, target, num_classes)\n",
    "\n",
    "    score = 2 * (prec * rec) / (prec + rec)\n",
    "    score[torch.isnan(score)] = 0\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b651ec7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model in GPU version and we do not need to import torch_sparse_old because backward() function is back in latest version\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "import pdb\n",
    "from torch_geometric.utils import dense_to_sparse, f1_score\n",
    "#from gcn import GCNConv\n",
    "from torch_scatter import scatter_add\n",
    "import torch_sparse\n",
    "#import torch_sparse_old\n",
    "from torch_geometric.utils.num_nodes import maybe_num_nodes\n",
    "from torch_geometric.utils import remove_self_loops, add_self_loops\n",
    "\n",
    "class GTN(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_edge, num_channels, w_in, w_out, num_class, num_nodes, num_layers):\n",
    "        super(GTN, self).__init__()\n",
    "        self.num_edge = num_edge\n",
    "        self.num_channels = num_channels\n",
    "        self.num_nodes = num_nodes\n",
    "        self.w_in = w_in\n",
    "        self.w_out = w_out\n",
    "        self.num_class = num_class\n",
    "        self.num_layers = num_layers\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            if i == 0:\n",
    "                layers.append(GTLayer(num_edge, num_channels, num_nodes, first=True))\n",
    "            else:\n",
    "                layers.append(GTLayer(num_edge, num_channels, num_nodes, first=False))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.gcn = GCNConv(in_channels=self.w_in, out_channels=w_out)\n",
    "        self.linear1 = nn.Linear(self.w_out*self.num_channels, self.w_out)\n",
    "        self.linear2 = nn.Linear(self.w_out, self.num_class)\n",
    "\n",
    "    def normalization(self, H):\n",
    "        norm_H = []\n",
    "        for i in range(self.num_channels):\n",
    "            edge, value=H[i]\n",
    "            edge, value = remove_self_loops(edge, value)\n",
    "            deg_row, deg_col = self.norm(edge.detach(), self.num_nodes, value)\n",
    "            value = deg_col * value\n",
    "            norm_H.append((edge, value))\n",
    "        return norm_H\n",
    "\n",
    "    def norm(self, edge_index, num_nodes, edge_weight, improved=False, dtype=None):\n",
    "        if edge_weight is None:\n",
    "            edge_weight = torch.ones((edge_index.size(1), ),\n",
    "                                    dtype=dtype,\n",
    "                                    device=edge_index.device)\n",
    "        edge_weight = edge_weight.view(-1)\n",
    "        assert edge_weight.size(0) == edge_index.size(1)\n",
    "        row, col = edge_index\n",
    "        deg = scatter_add(edge_weight.clone(), col, dim=0, dim_size=num_nodes)\n",
    "        deg_inv_sqrt = deg.pow(-1)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "\n",
    "        return deg_inv_sqrt[row], deg_inv_sqrt[col]\n",
    "\n",
    "    def forward(self, A, X, target_x, target):\n",
    "        Ws = []\n",
    "        for i in range(self.num_layers):\n",
    "            if i == 0:\n",
    "                H, W = self.layers[i](A)\n",
    "            else:                \n",
    "                H, W = self.layers[i](A, H)\n",
    "            H = self.normalization(H)\n",
    "            Ws.append(W)\n",
    "        for i in range(self.num_channels):\n",
    "            if i==0:\n",
    "                edge_index, edge_weight = H[i][0], H[i][1]\n",
    "                X_ = self.gcn(X,edge_index=edge_index.detach(), edge_weight=edge_weight)\n",
    "                X_ = F.relu(X_)\n",
    "            else:\n",
    "                edge_index, edge_weight = H[i][0], H[i][1]\n",
    "                X_ = torch.cat((X_,F.relu(self.gcn(X,edge_index=edge_index.detach(), edge_weight=edge_weight))), dim=1)\n",
    "        X_ = self.linear1(X_)\n",
    "        X_ = F.relu(X_)\n",
    "        y = self.linear2(X_[target_x])\n",
    "        loss = self.loss(y, target)\n",
    "        return loss, y, Ws\n",
    "\n",
    "class GTLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, num_nodes, first=True):\n",
    "        super(GTLayer, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.first = first\n",
    "        self.num_nodes = num_nodes\n",
    "        if self.first == True:\n",
    "            self.conv1 = GTConv(in_channels, out_channels, num_nodes)\n",
    "            self.conv2 = GTConv(in_channels, out_channels, num_nodes)\n",
    "        else:\n",
    "            self.conv1 = GTConv(in_channels, out_channels, num_nodes)\n",
    "    \n",
    "    def forward(self, A, H_=None):\n",
    "        if self.first == True:\n",
    "            result_A = self.conv1(A)\n",
    "            result_B = self.conv2(A)                \n",
    "            W = [(F.softmax(self.conv1.weight, dim=1)).detach(),(F.softmax(self.conv2.weight, dim=1)).detach()]\n",
    "        else:\n",
    "            result_A = H_\n",
    "            result_B = self.conv1(A)\n",
    "            W = [(F.softmax(self.conv1.weight, dim=1)).detach()]\n",
    "        H = []\n",
    "        for i in range(len(result_A)):\n",
    "            a_edge, a_value = result_A[i]\n",
    "            b_edge, b_value = result_B[i]\n",
    "            \n",
    "            edges, values = torch_sparse.spspmm(a_edge, a_value, b_edge, b_value, self.num_nodes, self.num_nodes, self.num_nodes)\n",
    "            H.append((edges, values))\n",
    "        return H, W\n",
    "\n",
    "class GTConv(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, num_nodes):\n",
    "        super(GTConv, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.weight = nn.Parameter(torch.Tensor(out_channels,in_channels))\n",
    "        self.bias = None\n",
    "        self.num_nodes = num_nodes\n",
    "        self.reset_parameters()\n",
    "    def reset_parameters(self):\n",
    "        n = self.in_channels\n",
    "        nn.init.normal_(self.weight, std=0.01)\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, A):\n",
    "        filter = F.softmax(self.weight, dim=1)\n",
    "        num_channels = filter.shape[0]\n",
    "        results = []\n",
    "        for i in range(num_channels):\n",
    "            for j, (edge_index,edge_value) in enumerate(A):\n",
    "                if j == 0:\n",
    "                    total_edge_index = edge_index\n",
    "                    total_edge_value = edge_value*filter[i][j]\n",
    "                else:\n",
    "                    total_edge_index = torch.cat((total_edge_index, edge_index), dim=1)\n",
    "                    total_edge_value = torch.cat((total_edge_value, edge_value*filter[i][j]))\n",
    "            index, value = torch_sparse.coalesce(total_edge_index.detach(), total_edge_value, m=self.num_nodes, n=self.num_nodes)\n",
    "            results.append((index, value))\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d48212",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ddbcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#main_sparse.py. delete the argparser to be able to run in notebook with run_main_sparse()\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from model_sparse import GTN\n",
    "from matplotlib import pyplot as plt\n",
    "import pdb\n",
    "from torch_geometric.utils import dense_to_sparse, f1_score, accuracy\n",
    "from torch_geometric.data import Data\n",
    "import torch_sparse\n",
    "import pickle\n",
    "#from mem import mem_report\n",
    "from scipy.sparse import csr_matrix\n",
    "import scipy.sparse as sp\n",
    "import argparse\n",
    "\n",
    "def run_main_sparse(dataset, epoch=40, node_dim=64, num_channels=2, lr=0.005, weight_decay=0.001, num_layers=3, norm=True, adaptive_lr=False):\n",
    "    epochs = epoch\n",
    "    node_dim = node_dim\n",
    "    num_channels = num_channels\n",
    "    lr = lr\n",
    "    weight_decay = weight_decay\n",
    "    num_layers = num_layers\n",
    "    norm = norm\n",
    "    adaptive_lr = adaptive_lr\n",
    "\n",
    "    with open('data/'+dataset+'/node_features.pkl','rb') as f:\n",
    "        node_features = pickle.load(f)\n",
    "    with open('data/'+dataset+'/edges.pkl','rb') as f:\n",
    "        edges = pickle.load(f)\n",
    "    with open('data/'+dataset+'/labels.pkl','rb') as f:\n",
    "        labels = pickle.load(f)\n",
    "        \n",
    "        \n",
    "    num_nodes = edges[0].shape[0]\n",
    "    A = []\n",
    "    \n",
    "    for i,edge in enumerate(edges):\n",
    "        edge_tmp = torch.from_numpy(np.vstack((edge.nonzero()[0], edge.nonzero()[1]))).type(torch.cuda.LongTensor)\n",
    "        value_tmp = torch.ones(edge_tmp.shape[1]).type(torch.cuda.FloatTensor)\n",
    "        A.append((edge_tmp,value_tmp))\n",
    "    edge_tmp = torch.stack((torch.arange(0,num_nodes),torch.arange(0,num_nodes))).type(torch.cuda.LongTensor)\n",
    "    value_tmp = torch.ones(num_nodes).type(torch.cuda.FloatTensor)\n",
    "    A.append((edge_tmp,value_tmp))\n",
    "\n",
    "    node_features = torch.from_numpy(node_features).type(torch.cuda.FloatTensor)\n",
    "    train_node = torch.from_numpy(np.array(labels[0])[:,0]).type(torch.cuda.LongTensor)\n",
    "    train_target = torch.from_numpy(np.array(labels[0])[:,1]).type(torch.cuda.LongTensor)\n",
    "\n",
    "    valid_node = torch.from_numpy(np.array(labels[1])[:,0]).type(torch.cuda.LongTensor)\n",
    "    valid_target = torch.from_numpy(np.array(labels[1])[:,1]).type(torch.cuda.LongTensor)\n",
    "    test_node = torch.from_numpy(np.array(labels[2])[:,0]).type(torch.cuda.LongTensor)\n",
    "    test_target = torch.from_numpy(np.array(labels[2])[:,1]).type(torch.cuda.LongTensor)\n",
    "\n",
    "\n",
    "    num_classes = torch.max(train_target).item()+1\n",
    "\n",
    "    train_losses = []\n",
    "    train_f1s = []\n",
    "    val_losses = []\n",
    "    test_losses = []\n",
    "    val_f1s = []\n",
    "    test_f1s = []\n",
    "    final_f1 = 0\n",
    "    for cnt in range(5):\n",
    "        best_val_loss = 10000\n",
    "        best_test_loss = 10000\n",
    "        best_train_loss = 10000\n",
    "        best_train_f1 = 0\n",
    "        best_val_f1 = 0\n",
    "        best_test_f1 = 0\n",
    "        model = GTN(num_edge=len(A),\n",
    "                        num_channels=num_channels,\n",
    "                        w_in = node_features.shape[1],\n",
    "                        w_out = node_dim,\n",
    "                        num_class=num_classes,\n",
    "                        num_nodes = node_features.shape[0],\n",
    "                        num_layers= num_layers)\n",
    "        model.cuda()\n",
    "        if adaptive_lr == 'false':\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "        else:\n",
    "            optimizer = torch.optim.Adam([{'params':model.gcn.parameters()},\n",
    "                                        {'params':model.linear1.parameters()},\n",
    "                                        {'params':model.linear2.parameters()},\n",
    "                                        {\"params\":model.layers.parameters(), \"lr\":0.5}\n",
    "                                        ], lr=0.005, weight_decay=0.001)\n",
    "        loss = nn.CrossEntropyLoss()\n",
    "        Ws = []\n",
    "        for i in range(50):\n",
    "            print('Epoch: ',i+1)\n",
    "            for param_group in optimizer.param_groups:\n",
    "                if param_group['lr'] > 0.005:\n",
    "                    param_group['lr'] = param_group['lr'] * 0.9\n",
    "            model.train()\n",
    "            model.zero_grad()\n",
    "            loss, y_train, _ = model(A, node_features, train_node, train_target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_f1 = torch.mean(f1_score(torch.argmax(y_train,dim=1), train_target, num_classes=3)).cpu().numpy()\n",
    "            print('Train - Loss: {}, Macro_F1: {}'.format(loss.detach().cpu().numpy(), train_f1))\n",
    "            model.eval()\n",
    "            # Valid\n",
    "            with torch.no_grad():\n",
    "                val_loss, y_valid,_ = model.forward(A, node_features, valid_node, valid_target)\n",
    "                val_f1 = torch.mean(f1_score(torch.argmax(y_valid,dim=1), valid_target, num_classes=3)).cpu().numpy()\n",
    "                print('Valid - Loss: {}, Macro_F1: {}'.format(val_loss.detach().cpu().numpy(), val_f1))\n",
    "                test_loss, y_test,W = model.forward(A, node_features, test_node, test_target)\n",
    "                test_f1 = torch.mean(f1_score(torch.argmax(y_test,dim=1), test_target, num_classes=3)).cpu().numpy()\n",
    "                test_acc = accuracy(torch.argmax(y_test,dim=1), test_target)\n",
    "                print('Test - Loss: {}, Macro_F1: {}, Acc: {}\\n'.format(test_loss.detach().cpu().numpy(), test_f1, test_acc))\n",
    "                if val_f1 > best_val_f1:\n",
    "                    best_val_loss = val_loss.detach().cpu().numpy()\n",
    "                    best_test_loss = test_loss.detach().cpu().numpy()\n",
    "                    best_train_loss = loss.detach().cpu().numpy()\n",
    "                    best_train_f1 = train_f1\n",
    "                    best_val_f1 = val_f1\n",
    "                    best_test_f1 = test_f1\n",
    "            torch.cuda.empty_cache()\n",
    "        print('---------------Best Results--------------------')\n",
    "        print('Train - Loss: {}, Macro_F1: {}'.format(best_test_loss, best_train_f1))\n",
    "        print('Valid - Loss: {}, Macro_F1: {}'.format(best_val_loss, best_val_f1))\n",
    "        print('Test - Loss: {}, Macro_F1: {}'.format(best_test_loss, best_test_f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818c35e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#python main_sparse.py --dataset IMDB --num_layers 3 --adaptive_lr true\n",
    "run_main_sparse(dataset='IMDB', num_layers=3, adaptive_lr=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b152818b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
